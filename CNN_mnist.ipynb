{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_mnist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The spatial features of a 2D image are lost when it s flattened to a 1D vector input.\n",
        "*   Parameter Issue -Number of parameters for the network increases if we flatten the input image as it is.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   CNN learns to recognize basic lines and curves, then shape and blobs and then increasingly complex objects within the image.It classifies the image by combining the larger,more complex objects.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0s3OJQ-kcc84"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG6yiZvWGnO1",
        "outputId": "1469e3ca-bbbe-46d9-9a5a-e40f19bb53ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mnist database has training set of 60000 examples\n",
            "The mnist database has training set of 60000 examples\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train,Y_train),(X_test,Y_test) = mnist.load_data()\n",
        "\n",
        "print(f\"The mnist database has training set of {len(X_train)} examples\")\n",
        "print(f\"The mnist database has training set of {len(X_train)} examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the Training Images\n",
        "\n"
      ],
      "metadata": {
        "id": "O_o4x_PceJNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "\n",
        "for it in range(10):\n",
        "  ax = fig.add_subplot(1,10,it+1,xticks=[],yticks=[])\n",
        "  ax.imshow(X_train[it],cmap='gray')\n",
        "  ax.set_title(str(Y_train[it]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "LMd9TD6_eEnx",
        "outputId": "081f8bdb-8cb5-4728-ad26-c708f644c53f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAACACAYAAACx+5SIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfrklEQVR4nO3de7zNVf7H8c8qEumkMF00UW4xQklFfpiQklCGyCVdRsNQzQyjKVMKkW4PlFBJ4THlMbmk0ZQJ6erBGP0eksIUaQhFbjlovr8/jt/qs1bnHHvvs/f5fvd3v56Ph8fj/bW+57tX7fPdl2Wt9TFBEAgAAAAAAACi5biwOwAAAAAAAICfYtAGAAAAAAAgghi0AQAAAAAAiCAGbQAAAAAAACKIQRsAAAAAAIAIYtAGAAAAAAAgghi0AQAAAAAAiKDYD9oYY5YaYw4aY/Yd/fNp2H1Ccowxpxlj5hpj9htjNhljbgy7T0idMab20XtyZth9QXKMMYOMMSuNMfnGmOlh9wepMcbUM8YsNsZ8Z4zZYIy5Luw+ITnGmHLGmOeOvifuNcasNsZcHXa/kDheT+PBGDPTGLPVGLPHGPOZMea2sPuE5HAvxktcv2fEftDmqEFBEFQ8+qdu2J1B0p4SkUMicrqI9BKRp40xvwi3SyiBp0RkRdidQEr+IyKjRGRa2B1BaowxZURkvoi8JiKniUh/EZlpjKkTaseQrDIi8qWItBKRU0RkuIjMNsbUCLFPSA6vp/EwRkRqBEGQJyKdRGSUMaZJyH1CcrgX4yWW3zNyZdAGWcoYc5KIdBWRPwdBsC8IgndF5FUR6RNuz5AKY0wPEdktIm+F3RckLwiCOUEQzBORb8LuC1J2voicJSJPBEHwQxAEi0XkPeE1NasEQbA/CIIRQRB8EQTBf4MgeE1EPhcRvixmCV5P4yEIgo+DIMj//8Ojf2qG2CUkiXsxPuL8PSNXBm3GGGN2GmPeM8a0DrszSEodETkSBMFn6u8+EhFm2mQZY0yeiDwoIr8Puy8AHEZEGoTdCaTOGHO6FLxffhx2X4BcY4yZZIw5ICLrRGSriCwMuUtAzon794xcGLQZJiLniUg1EZkqIguMMYyAZ4+KIrLH+7vvROTkEPqCkhkpIs8FQbAl7I4AOexTEdkuIkONMWWNMVdKwRKbCuF2C6kyxpQVkVki8kIQBOvC7g+Qa4IgGCgFn0v/R0TmiEh+8T8BIANi/T0j9oM2QRAsD4JgbxAE+UEQvCAF08A7hN0vJGyfiOR5f5cnIntD6AtSZIxpLCJtReSJsPsC5LIgCA6LSBcRuUZEtonIH0RktojE8kNO3BljjhORGVKw79ugkLsD5Kyjy03fFZGzRWRA2P0BckkufM8oE3YHQhBIwVRwZIfPRKSMMaZ2EATrj/5dI2EKeLZpLSI1RGSzMUakYAbV8caY+kEQXBRiv4CcEwTB/0rB7BoRETHGvC8iL4TXI6TCFLyYPicFm/R3ODogByBcZYQ9bYDS1lpi/j0j1jNtjDGVjDHtjTEnGmPKGGN6iUhLEfl72H1DYoIg2C8FU00fNMacZIy5XEQ6S8G/LCJ7TJWCDzGNj/6ZLCJ/E5H2YXYKyTn6OnqiiBwvBW+GJx6tRoQsYoxpePS5q2CMGSIiZ4rI9JC7heQ9LSL1ROTaIAi+D7szSA6vp9nPGPMzY0wPY0xFY8zxxpj2ItJTYrgJapxxL8ZC7L9nxHrQRkTKSkEJtx0islNEBotIF29TW0TfQBEpLwX7MPxFRAYEQcBMmywSBMGBIAi2/f8fKVj2djAIgh1h9w1JGS4i34vI3SLS+2geHmqPkIo+UrBZ5nYRaSMi7VT1E2QBY0x1EbldCj6cbjPG7Dv6p1fIXUPieD3NfoEULIXaIiK7RORREbkrCIJXQ+0VksW9mOVy4XuGCYIg7D4AAAAAAADAE/eZNgAAAAAAAFmJQRsAAAAAAIAIYtAGAAAAAAAgghi0AQAAAAAAiCAGbQAAAAAAACIoqRr0xhhKTYUkCAKTjuvwHIZqZxAEVdNxIZ7H8HAvxgL3YgxwL8YC92IMcC/GAvdiDHAvxkKh9yIzbYDSsynsDgAQEe5FICq4F4Fo4F4EoqHQe5FBGwAAAAAAgAhi0AYAAAAAACCCGLQBAAAAAACIIAZtAAAAAAAAIohBGwAAAAAAgAhi0AYAAAAAACCCGLQBAAAAAACIIAZtAAAAAAAAIohBGwAAAAAAgAhi0AYAAAAAACCCGLQBAAAAAACIoDJhdwBIVZMmTWweNGiQ09a3b1+bX3zxRZsnTpzonLdq1aoM9Q4AAOBH48ePt/mOO+6wec2aNc55HTt2tHnTpk2Z7xgAICVvvfWWzcYYm6+44oq0Pg4zbQAAAAAAACKIQRsAAAAAAIAIit3yqOOPP97mU045JaGf8ZfWVKhQwea6deva/Nvf/tY579FHH7W5Z8+eTtvBgwdtHjt2rM0PPPBAQn3CTzVu3Ng5XrRokc15eXlOWxAENvfp08fmTp06OedVrlw5nV1ESNq0aWPzrFmznLZWrVrZ/Omnn5Zan/BTw4cPt9l/LTzuuB//DaF169ZO29tvv53RfgFxcfLJJ9tcsWJFp+2aa66xuWrVqjY//vjjznn5+fkZ6l3uqVGjhnPcu3dvm//73//aXK9ePee8888/32aWR4WrTp06znHZsmVtbtmypc2TJk1yztPPb6rmz59vc48ePZy2Q4cOlfj6uUw/j82bN7f5oYcecs67/PLLS61PyA5PPPGEc6x/f/SWHOnGTBsAAAAAAIAIYtAGAAAAAAAggiK7POqcc85xjk844QSb9TSkFi1aOOdVqlTJ5q5du5a4H1u2bLF5woQJTtt1111n8969e522jz76yGam9qfukksusfmVV15x2vTyN70cSsR9PvQUUn851GWXXWazX0kqjlNP9VRe/f9i7ty5YXQnbZo2bWrzihUrQuwJfP369bN52LBhNhc3ddy/nwH8SC+50feUiEizZs1sbtCgQULXO/PMM51jXdUIJbNjxw7neNmyZTb7y7URrl/84hc26/etbt26OefppbxnnXWWzf57Wjrex/TvyOTJk522u+66y+Y9e/aU+LFyjf4OsWTJEpu3bdvmnHfGGWcU2Ybcobc6+c1vfuO0HT582GZdSSrdmGkDAAAAAAAQQQzaAAAAAAAARBCDNgAAAAAAABEUqT1tdEnnxYsXO22Jlu9OB70uVZeo3bdvn3OeLi28detWp23Xrl02U2a4eLrEuojIRRddZPPMmTNt9tfdF2f9+vU2jxs3zuaXXnrJOe+9996zWT/XIiJjxoxJ+PGyhS6lXLt2bZuzbU8bvaZcROTcc8+1uXr16k6bMaZU+oTC6efjxBNPDLEnuevSSy+1WZccbtWqlXOe3tPBN2TIEJv/85//2OzvK6dfs5cvX558ZyEibslnEXf/il69etlcvnx55zz9evfll186bXqvN11iunv37s55unTxunXrkuk2PPv373eOKd8dXfozX4cOHULsSeH69u3rHD/33HM268+yKBm9h41/zJ42uUvvgarLxYuIvPvuuzbPnj07Y31gpg0AAAAAAEAEMWgDAAAAAAAQQZFaHrV582abv/nmG6etpMuj/Gnau3fvtvmXv/yl06ZLPc+YMaNEj4tjmzJlinPcs2fPEl9TL7GqWLGizX75db1cqGHDhiV+3KjT02s/+OCDEHtSMv5SuV//+tc26+UZIkzvL21t27Z1jgcPHlzoef7z0rFjR5u//vrr9Hcsh9xwww3O8fjx422uUqWKzf7SwaVLl9pctWpVp+2RRx4p9LH8a+if69GjR2IdzmH6s83DDz9ss/8cnnzyyQldTy8Nbt++vdOmp3Tr+0//ThR2jNRVqlTJOW7UqFFIPcGxLFq0yObilkdt377dZr1EyV+27ZcA15o3b26zv0wV4WJJffZo2bKlzffee6/N/vfIb7/9Nulr+9do0KCBzRs3bnTa9PLxTGKmDQAAAAAAQAQxaAMAAAAAABBBDNoAAAAAAABEUKT2tNFrzoYOHeq06f0O/vWvf9k8YcKEIq+3evVqm9u1a+e06TKMfpnTO++8M8EeI1VNmjSx+ZprrnHailpP6u9Hs2DBApsfffRRp02XpNW/L7oUu4jIFVdccczHjRN/zXW2evbZZ4ts03s6oHToss/PP/+801bUfmT+HimUwk1emTI/voVffPHFNj/zzDPOeRUqVLB52bJlNo8cOdI5T5etLFeunNOmy1heeeWVRfZp5cqVx+o2lOuuu87m2267Lemf99fW6886fsnvWrVqJX19lIy+90REzjnnnIR+rmnTpjb7+3/xWpkZTz/9tM3z5s0r8rzDhw/bnGoJ6Ly8PJvXrFlj81lnnVXkz/h94rU2M4IgcI5PPPHEkHqCY5k6darNtWvXtrl+/frOefqzTaLuuece57hy5co26300RUQ++uijpK+finh8gwMAAAAAAIgZBm0AAAAAAAAiKFLLozR/GuDixYtt3rt3r81++cRbb73VZr1kRi+H8n388cfOcf/+/ZPrLBLSuHFjm3VpRT1NVMSdmvj666/b7Jdf02UShw8f7rTp5TM7duyw2Z/Cpksy+su0dNnwVatWSTbyy5iffvrpIfUkvYpaciPi/m6hdNx00002Fze9W5eUfvHFFzPZpZzQu3dvm4tbMqjvCV1Kes+ePUX+jF9yuqglUVu2bHGOX3jhhSKviZ/q1q1bQud98cUXNq9YscLmYcOGOef5S6K0evXqJdc5lJheqi0iMn36dJtHjBhR5M/ptt27dzttTz75ZDq6Bs+RI0dsLu4+Sof27dvbfOqppyb0M/5rbX5+flr7hMLppccffvhhiD2B78CBAzbr746pLmnT31OrV6/utOnvi2EtmWOmDQAAAAAAQAQxaAMAAAAAABBBkV0e5StqGvd3331X5M/o3Z1ffvllp01Pc0Jm1KlTxznWFcH08padO3c6523dutVmPdV+3759znl/+9vfCs2pKl++vHP8hz/8weZevXqV+Pph6NChg3Ps/zdmE72069xzzy3yvK+++qo0upPTqlSp4hzfcsstNvuvrXpq/6hRozLbsZjzqz3p6gZ6avCkSZOc8/Ty0eKWRGn33ntvQufdcccdzrFejopj059T9NLsN9980zlvw4YNNm/fvj2lx4rL8thspu/h4pZHIV569OjhHOv7PtHPZffdd19a+5Tr9HI4/V3SX35fs2bNUusTiud/Brrgggts/uSTT2xOpprTSSedZLNebuxX/tNL4/76178mfP10YqYNAAAAAABABDFoAwAAAAAAEEEM2gAAAAAAAERQ1uxpUxR/TXCTJk1s1iWh27Zt65znrxdHepQrV85mXXJdxN1fRZdt79u3r3PeypUrbQ5zD5ZzzjkntMdOl7p16xbZ5pe6jzr9++TvzfDZZ5/ZrH+3kD41atSw+ZVXXkn45yZOnGjzkiVL0tmlnKD3MdB72IiIHDp0yOY33njDZr8M9Pfff1/otf2ylbqst//6Z4yxWe9NNH/+/CL7jmPTJaEzvcdJs2bNMnp9JOe44378d1P2Wcx+/t6Hd999t821atVy2sqWLZvQNVevXm3z4cOHS9A7+PR+e++8847NHTt2DKM7KMLPf/5zm/VeUCLuvkSDBg2yOZm99R5//HGbu3XrZrN+bxYRufzyyxO+ZqYw0wYAAAAAACCCGLQBAAAAAACIoKxfHrV//37nWE+dWrVqlc3PPPOMc56epq+X44iIPPXUUzbrMqo4tgsvvNBmv9y01rlzZ5vffvvtjPYJhVuxYkXYXRARkby8PJuvuuoqp613794266UbPl0GUE95Rfro56Zhw4ZFnvfWW285x+PHj89Yn+KoUqVKzvHAgQNt9t+P9JKoLl26JHR9PU1/1qxZTpteXuzTJS7HjRuX0GMhM3SZdV2u9Fh0eVTt/fffd44/+OCD1DqGpOglUXzWDJ9eAtynTx+b/e0VitKiRQvnONHndM+ePTbrJVUiIgsXLrS5qGWuQNw0aNDA5rlz59pcpUoV5zy9/D7R75JDhgxxjvv161foeaNHj07oeqWJmTYAAAAAAAARxKANAAAAAABABGX98ijfxo0bbdZTnp5//nnnPD31UWcRd7rxiy++aPPWrVvT1c3Y0rtw62ojIu7Utagsicrl6g2nnXZaSj/XqFEjm/Vz7E8hPvvss20+4YQTbPYrLOjnwJ/+u3z5cpvz8/NtLlPGfen65z//mVDfkRy95Gbs2LFFnvfuu+/afNNNNzlt3333Xfo7FmP6XhH56XRgTS+T+dnPfmbzzTff7JzXqVMnm/W044oVKzrn6en8/tT+mTNn2uwvS0Z6VKhQweb69es7bffff7/NxS09TvQ9TVfG8H9ffvjhh2N3Fshy+rVQROTVV1+1uTSrh+rKRVOnTi21x0ViKleuHHYXYkl/jtdbIYiIPPfcczYX956mKyL+6U9/sll/FxVxv+/oClEi7vcY/Z1/ypQpxf8HhICZNgAAAAAAABHEoA0AAAAAAEAEMWgDAAAAAAAQQbHb00bTZcLWr1/vtOn1bm3atHHaHnroIZurV69us1/+66uvvkpLP7NZx44dnePGjRvb7O+JoNcLR0VxJTdXr15d2t1JO3+PGP3fOHnyZJvvueeehK+pyz3rtaBHjhxxzjtw4IDNa9eutXnatGnOeStXrrTZ3+vo66+/tnnLli02ly9f3jlv3bp1CfUdxdMlT0VEXnnllYR+7t///rfN+jlD8g4dOuQc79ixw+aqVas6bZ9//rnNiZaX1XuZ6FKzIiJnnnmmzTt37nTaFixYkND1UbyyZcs6xxdeeKHN+n7Tz4WI+1qun0O/PPdVV11ls94jx6f3E7j++uudtvHjx9vs/z4CcaU/z/h7MiZC770hkvg+ifpz9NVXX+20vf7660n3A+ml94RD+vTo0cPmZ5991mnTn2f0fbRhwwbnvIsvvrjQ3LlzZ+e8atWq2ey/t+rPWLfccktCfQ8LM20AAAAAAAAiiEEbAAAAAACACIr18ihtzZo1znH37t1tvvbaa502XR789ttvt7l27drOee3atUtnF7OSv0xFl6vdvn270/byyy+XSp985cqVs3nEiBFFnrd48WLnWJePy1YDBw50jjdt2mRz8+bNU7rm5s2bbZ43b57Nn3zyiXPehx9+mNL1tf79+9usl4bo5ThIn2HDhjnHiU7vLq4cOJKze/du51iXXX/ttdecNl3GcuPGjTbPnz/fOW/69Ok2f/vttza/9NJLznl62rDfhtTp90W9fElEZM6cOYX+zAMPPOAc6/en9957z2b9O+Cf55c01vTr6ZgxY5y2ol7jRUTy8/OLvCaSk2h59pYtWzrHTz75ZMb6lEv87wWtW7e2WZcgfuONN5zzDh48mPRj3Xrrrc7x4MGDk74GMmfJkiU2+9s+ID1uuOEG51h/1z58+LDTpj8H3XjjjTbv2rXLOe+xxx6zuVWrVjbrpVIi7nJHfyl5lSpVbP7yyy9t1q8HIu5nrLAw0wYAAAAAACCCGLQBAAAAAACIIAZtAAAAAAAAIihn9rTx6fVyM2bMcNp06TFdFtNfV6zXuy1dujS9HYwBf+371q1bS+2x9T42w4cPt3no0KHOebqMtF4bKSKyb9++DPUuPA8//HDYXUhKmzZtCv37REtR49gaN25s85VXXpnQz/h7pnz66adp7RN+tHz5cpv9kt+p0O9jeg24iLuvBvtGpc4v6633p/HfgzRd3nfixIlOm/7Mon8PFi5c6Jx3wQUX2OyX6x43bpzNer8bvzzqrFmzbP7HP/7htOn3EH9/AW316tVFtqGAvt/8fRY0vyR7/fr1bV67dm36O5aj9J5/o0ePTuu1/f0U2dMmWvQ+Xj79el69enWnTf/OoHh6j1gR9//5qFGjnDa9301x9H00ZcoUm5s1a5Zwv/R+N3pvoyjsYeNjpg0AAAAAAEAEMWgDAAAAAAAQQTmzPKphw4bO8a9+9SubmzZt6rTpJVGaPw112bJlaepdPL366qul9lh6iYeIOwVdl5nzl3V07do1sx1DRsydOzfsLsTGm2++afOpp55a5Hm6hHu/fv0y2SVkUPny5W32ywzrJRqU/E7O8ccfb/PIkSOdtiFDhti8f/9+p+3uu++2Wf8/90u/6xKmuuTzhRde6Jy3fv16mwcMGOC06anfeXl5Njdv3tw5r1evXjZ36tTJaVu0aJEURpdKFRE599xzCz0PP5o8ebLN/tKB4vTv39/mu+66K619Qma0b98+7C6gGEeOHCmyTS+f0VsvIDn+9685c+bY7L9/JEqX69ZLfn09e/a0ec2aNUWep7fMiCJm2gAAAAAAAEQQgzYAAAAAAAARFLvlUXXr1rV50KBBNvu7759xxhkJXe+HH36w2a9+5E8tz0V62qB/3KVLF6ftzjvvTOtj/+53v7P5z3/+s9N2yimn2KwrYfTt2zetfQCyXeXKlW0u7jVt0qRJNsexslqueOONN8LuQizpJSt6OZSIyIEDB2z2l8Ho5YmXXXaZzTfffLNz3tVXX22zXuL24IMPOufpqhvFTTnfs2ePzX//+9+dNn2sp5WLiNx4442FXk+/HyMx69atC7sLsedXctMVEhcvXuy0ff/992l9bH0Pjx8/Pq3XRnrppTv+fXn++efb7C9HHDhwYGY7FiPpuAf0dzsRkW7dutmsl/z6lZ9mz55d4seOAmbaAAAAAAAARBCDNgAAAAAAABHEoA0AAAAAAEAEZeWeNno/Gn+9td7HpkaNGildf+XKlTaPHj3a5tIsYZ0tdIlY/9jfN2jChAk2T5s2zeZvvvnGOU+v6+/Tp4/NjRo1cs47++yzbd68ebPTpvdt0HtxIHvp/ZLq1KnjtOly1Dg2ve/FccclNnb//vvvZ6o7KEWUns2M++67r8g2XQ586NChTtuIESNsrlWrVkKPpX9mzJgxTpvehy8d/vKXvxR7jNRNnDjR5sGDBzttNWvWLPLn9P6A+hr+Pg65qkWLFjbfe++9Tlu7du1s9svSp1J2+LTTTrO5Q4cOTtvjjz9uc4UKFYq8ht5L5+DBg0n3Aeml9xkTEalWrZrNv//970u7O1D8PYQGDBhg8/bt222+4oorSq1PpYmZNgAAAAAAABHEoA0AAAAAAEAERXZ51Omnn+4c169f3+Ynn3zSZl2KLRnLly+3+ZFHHnHadOk3ynqnTk8JF3GntXXt2tVmXXpURKR27doJXV8v11iyZInTVtxUdWQnvfQu0SU9KNC4cWPnuG3btjbr17hDhw455z311FM2f/311xnqHUrTeeedF3YXYmnbtm02V61a1WkrV66czf4yX23hwoU2L1u2zGmbN2+ezV988YXN6V4OhXB8/PHHznFx9ymfS4unvyM0aNCgyPP++Mc/Osd79+5N+rH0cquLLrrIafO3D9CWLl1q89NPP22z/1kW4dPPo/8ZCZlXvXp1m2+77TanTT83U6dOtXnLli2Z71gI+OYDAAAAAAAQQQzaAAAAAAAARBCDNgAAAAAAABEU6p42ulSeiMiUKVNs9vdgSGUdvt7z5LHHHnPadEloXW4Pyfnggw+c4xUrVtjctGnTIn9OlwP39y/SdDnwl156yWnTZS+RW5o1a+YcT58+PZyOZIlKlSo5x/r+07766ivneMiQIRnrE8Lxzjvv2OzvDcVeGalr2bKlzV26dHHa9F4XuiypiMi0adNs3rVrl83snZBb9H4MIiLXXnttSD3JHbpccCboe33BggVOm/78SpnvaMvLy7O5c+fOTtvcuXNLuzs5Z9GiRTbr/W1ERGbOnGnz/fffX2p9CgszbQAAAAAAACKIQRsAAAAAAIAIKpXlUZdeeqnNQ4cOtfmSSy5xzqtWrVrS1z5w4IBzPGHCBJsfeughm/fv35/0tXFsflm166+/3ubbb7/daRs+fHhC1xw/frzNuhTihg0bUukiYsIYE3YXgKy3Zs0am9evX++06WXINWvWdNp27NiR2Y5lOV0ueMaMGU6bfwz41q5d6xx/8sknNterV6+0u5PV+vXrZ/PgwYOdtptuuqnE19+4caPN+juIXnoq4i5506+7iLbu3bs7x/n5+Tbr+xKl4/nnn7d55MiRTtv8+fNLuzuhYqYNAAAAAABABDFoAwAAAAAAEEEmCILETzYm8ZOVsWPH2qyXRxXHnyr62muv2XzkyBGb/apQu3fvTqWLkRcEQVrWhqT6HCIt/hkEwcXpuFCuPI96mrOusvLMM8845/lL8TIpG+9Fv1rUyy+/bHOLFi1s/vzzz53zatWqldmOhYd7Udz7S0Tk2Weftfntt9922vQyA//9OSzZeC/iJ7gXYyCq92K5cuWcY/2aN2rUKKft1FNPtXnevHk26+o1Iu6SjG3btqWjm1HBvSg/rVSrlyd26tTJadu0aVOp9CkZUb0XkZRC70Vm2gAAAAAAAEQQgzYAAAAAAAARxKANAAAAAABABJXKnjYoOdYoxgLrhWOAezEWuBdFJC8vzzmePXu2zW3btnXa5syZY/PNN99s8/79+zPUu2PjXowF7sUY4F6MBe7FGOBejAX2tAEAAAAAAMgWDNoAAAAAAABEUJmwOwAAAErfnj17nOPu3bvbPHr0aKdtwIABNo8YMcLmqJT/BgAAiCtm2gAAAAAAAEQQgzYAAAAAAAARxKANAAAAAABABFHyO0tQwi0WKKcYA9yLscC9GAPci7HAvRgD3IuxwL0YA9yLsUDJbwAAAAAAgGzBoA0AAAAAAEAEJVvye6eIbMpER1Cs6mm8Fs9heHgesx/PYTzwPGY/nsN44HnMfjyH8cDzmP14DuOh0OcxqT1tAAAAAAAAUDpYHgUAAAAAABBBDNoAAAAAAABEEIM2AAAAAAAAEcSgDQAAAAAAQAQxaAMAAAAAABBBDNoAAAAAAABEEIM2AAAAAAAAEcSgDQAAAAAAQAQxaAMAAAAAABBB/wfpJZTs74WtDQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Input Images ; Rescale the image by dividing every pixel in every image by 255"
      ],
      "metadata": {
        "id": "ZPUXI904gMyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rescale to have values within 0-1 range [0,255] --> [0,1]\n",
        "\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "\n",
        "print(f\"X_train shape : {X_train.shape}\")\n",
        "print(f\"{X_train.shape[0]} train examples\")\n",
        "print(f\"{X_test.shape[0]} test examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2U_dQmHeq92",
        "outputId": "6e193b91-4734-49cc-c873-50dd4957bec9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape : (60000, 28, 28)\n",
            "60000 train examples\n",
            "10000 test examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the Labels"
      ],
      "metadata": {
        "id": "uMFCQ3REhPrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# Print First Ten (integer - valued) training labels\n",
        "print('Integer valued labels:')\n",
        "print(Y_train[:10])\n",
        "\n",
        "#One hot encode the labels\n",
        "#Convert class vectors to binary class matrices\n",
        "\n",
        "Y_train = np_utils.to_categorical(Y_train,num_classes)\n",
        "Y_test = np_utils.to_categorical(Y_test,num_classes)\n",
        "\n",
        "#Print first ten (one-hot) training labels\n",
        "print('One-ht labels')\n",
        "print(Y_train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRygVF3ahFI7",
        "outputId": "5d129124-a764-4865-8588-96d23a8fcb9e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integer valued labels:\n",
            "[5 0 4 1 9 2 1 3 1 4]\n",
            "One-ht labels\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reshape data to fit our CNN "
      ],
      "metadata": {
        "id": "ld_k9JHqr7Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imageRows , imageColumns = 28,28\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0],imageRows , imageColumns, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0],imageRows , imageColumns, 1)\n",
        "inputShape = (imageRows,imageColumns,1)\n",
        "\n",
        "print(f'input shape:{inputShape}')\n",
        "print(f'X_train shape:{X_train.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKJ9IkUXr1F6",
        "outputId": "8fc38e2a-95f6-4312-9df2-fd9bea7d1fce"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape:(28, 28, 1)\n",
            "X_train shape:(60000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model Architecture"
      ],
      "metadata": {
        "id": "7WMa_qABtAqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Always add a ReLU activation function to the Conv2D layers in CNN.with the exception of the final layer in the netowrk , Dense Layers should also have a ReLU activation function.\n",
        "*   When Constructing a network for classification, the final layer in the network should be a Dense Layer with a softmax activation function.The number of nodes in the final layer should equal to the total number of classes in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "NY5ZdMl6tIV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D , MaxPooling2D, Flatten , Dense , Dropout,GlobalAveragePooling2D\n",
        "\n",
        "# Create a Sequential Object\n",
        "modelCNN = Sequential()\n",
        "\n",
        "# CONV1: add CONV layer with reLU activation and depth =32 kernels.\n",
        "#Input shape is (28,28,1) because it is a gray scale image.\n",
        "#It has 1 channel.\n",
        "modelCNN.add(Conv2D(32,kernel_size = (3,3),padding = 'same',activation='relu',input_shape=(28,28,1))) # 28*28*32\n",
        "\n",
        "#POOL_1 downsample the image to choose the best features\n",
        "modelCNN.add(MaxPooling2D(pool_size=(2,2))) # 14*14*32\n",
        "\n",
        "#CONV_2 increase the depth to 64\n",
        "modelCNN.add(Conv2D(64,(3,3),padding='same',activation='relu')) # 14*14*64\n",
        "\n",
        "#POOL_2 more downsampling\n",
        "modelCNN.add(MaxPooling2D(pool_size=(2,2))) # 7*7*64\n",
        "\n",
        "# Flatten the layer\n",
        "modelCNN.add(Flatten()) # 3136\n",
        "\n",
        "# FC_1: Fully connected to get all the details\n",
        "modelCNN.add(Dense(64,activation='relu')) \n",
        "\n",
        "#3FC_2 : Output a softmax to squash the matrix into output probabilities for the 10 classes\n",
        "modelCNN.add(Dense(10,activation='softmax'))\n",
        "modelCNN.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKquQdISs6-Z",
        "outputId": "1dcbf724-cf4c-4246-929a-60007014511e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 14, 14, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 14, 14, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 7, 7, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 3136)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                200768    \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 220,234\n",
            "Trainable params: 220,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelCNN_2= Sequential()\n",
        "\n",
        "# CONV1: add CONV layer with reLU activation and depth =16 kernels.\n",
        "#Input shape is (28,28,1) because it is a gray scale image.\n",
        "#It has 1 channel.\n",
        "modelCNN_2.add(Conv2D(16,kernel_size = (3,3),padding = 'same',activation='relu',input_shape=(28,28,1))) # 28*28*16\n",
        "\n",
        "#POOL_1 downsample the image to choose the best features\n",
        "modelCNN_2.add(MaxPooling2D(pool_size=(2,2))) # 14*14*16\n",
        "\n",
        "#CONV_2 increase the depth to 32\n",
        "modelCNN_2.add(Conv2D(32,(3,3),padding='same',activation='relu')) # 14*14*32\n",
        "\n",
        "#POOL_2 more downsampling\n",
        "modelCNN_2.add(MaxPooling2D(pool_size=(2,2))) # 7*7*32\n",
        "\n",
        "# Flatten the layer\n",
        "modelCNN_2.add(Flatten()) # 1568\n",
        "\n",
        "# FC_1: Fully connected to get all the details\n",
        "modelCNN_2.add(Dense(32,activation='relu')) \n",
        "\n",
        "#3FC_2 : Output a softmax to squash the matrix into output probabilities for the 10 classes\n",
        "modelCNN_2.add(Dense(10,activation='softmax'))\n",
        "modelCNN_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEds1mI2I4YT",
        "outputId": "ba434abf-b834-48fa-ee72-150409e2320f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_11 (Conv2D)          (None, 28, 28, 16)        160       \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 14, 14, 16)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 14, 14, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 7, 7, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 1568)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                50208     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 55,338\n",
            "Trainable params: 55,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelCNN_3= Sequential()\n",
        "\n",
        "# CONV1: add CONV layer with reLU activation and depth =16 kernels.\n",
        "#Input shape is (28,28,1) because it is a gray scale image.\n",
        "#It has 1 channel.\n",
        "modelCNN_3.add(Conv2D(128,kernel_size = (3,3),padding = 'same',activation='relu',input_shape=(28,28,1))) # 28*28*128\n",
        "\n",
        "#POOL_1 downsample the image to choose the best features\n",
        "modelCNN_3.add(MaxPooling2D(pool_size=(2,2))) # 14*14*128\n",
        "\n",
        "#CONV_2 increase the depth to 32\n",
        "modelCNN_3.add(Conv2D(256,(3,3),padding='same',activation='relu')) # 14*14*256\n",
        "\n",
        "#POOL_2 more downsampling\n",
        "modelCNN_3.add(MaxPooling2D(pool_size=(2,2))) # 7*7*256\n",
        "\n",
        "# Flatten the layer\n",
        "modelCNN_3.add(Flatten()) # 12544\n",
        "\n",
        "# FC_1: Fully connected to get all the details\n",
        "modelCNN_3.add(Dense(32,activation='relu')) \n",
        "\n",
        "#3FC_2 : Output a softmax to squash the matrix into output probabilities for the 10 classes\n",
        "modelCNN_3.add(Dense(10,activation='softmax'))\n",
        "modelCNN_3.summary()"
      ],
      "metadata": {
        "id": "ZQixOfUdMKyF",
        "outputId": "abdf7b01-39f8-437a-d877-5ac91e89e946",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_13 (Conv2D)          (None, 28, 28, 128)       1280      \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 14, 14, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 14, 14, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  (None, 7, 7, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12544)             0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 32)                401440    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 698,218\n",
            "Trainable params: 698,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile the Model"
      ],
      "metadata": {
        "id": "ryhYsnOiFhXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelCNN.compile(loss ='categorical_crossentropy',optimizer = 'rmsprop',metrics = ['accuracy'])\n",
        "modelCNN_2.compile(loss ='categorical_crossentropy',optimizer = 'rmsprop',metrics = ['accuracy'])\n",
        "modelCNN_3.compile(loss ='categorical_crossentropy',optimizer = 'rmsprop',metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "3URPfWlBs_eu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "vfy_zUjfFwj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#train the model\n",
        "checkpointer1 = ModelCheckpoint(filepath='model.weights_CNN.best.hdf5',verbose=1,save_best_only=True)\n",
        "\n",
        "modelFit1 = modelCNN.fit(X_train,Y_train, batch_size = 64 , epochs = 10, validation_data = (X_test,Y_test),callbacks=[checkpointer1],verbose=2,shuffle=True)\n",
        "\n",
        "checkpointer2 = ModelCheckpoint(filepath='model.weights_CNN_2.best.hdf5',verbose=1,save_best_only=True)\n",
        "modelFit2 = modelCNN_2.fit(X_train,Y_train, batch_size = 64 , epochs = 10, validation_data = (X_test,Y_test),callbacks=[checkpointer2],verbose=2,shuffle=True)\n",
        "\n",
        "checkpointer3 = ModelCheckpoint(filepath='model.weights_CNN_3.best.hdf5',verbose=1,save_best_only=True)\n",
        "modelFit2 = modelCNN_3.fit(X_train,Y_train, batch_size = 64 , epochs = 10, validation_data = (X_test,Y_test),callbacks=[checkpointer3],verbose=2,shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOX10s4CFsMb",
        "outputId": "f9f64823-628b-42c9-c998-4bcbeb76f3f4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.03772, saving model to model.weights_CNN.best.hdf5\n",
            "938/938 - 4s - loss: 0.0200 - accuracy: 0.9941 - val_loss: 0.0377 - val_accuracy: 0.9888 - 4s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "\n",
            "Epoch 2: val_loss improved from 0.03772 to 0.03536, saving model to model.weights_CNN.best.hdf5\n",
            "938/938 - 4s - loss: 0.0160 - accuracy: 0.9950 - val_loss: 0.0354 - val_accuracy: 0.9902 - 4s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "\n",
            "Epoch 3: val_loss improved from 0.03536 to 0.03465, saving model to model.weights_CNN.best.hdf5\n",
            "938/938 - 4s - loss: 0.0124 - accuracy: 0.9961 - val_loss: 0.0347 - val_accuracy: 0.9902 - 4s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "\n",
            "Epoch 4: val_loss improved from 0.03465 to 0.02929, saving model to model.weights_CNN.best.hdf5\n",
            "938/938 - 4s - loss: 0.0105 - accuracy: 0.9968 - val_loss: 0.0293 - val_accuracy: 0.9919 - 4s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "\n",
            "Epoch 5: val_loss improved from 0.02929 to 0.02899, saving model to model.weights_CNN.best.hdf5\n",
            "938/938 - 4s - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.0290 - val_accuracy: 0.9928 - 4s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.02899\n",
            "938/938 - 4s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0296 - val_accuracy: 0.9924 - 4s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "\n",
            "Epoch 7: val_loss did not improve from 0.02899\n",
            "938/938 - 4s - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0331 - val_accuracy: 0.9926 - 4s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.02899\n",
            "938/938 - 4s - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0389 - val_accuracy: 0.9922 - 4s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "\n",
            "Epoch 9: val_loss did not improve from 0.02899\n",
            "938/938 - 4s - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.0467 - val_accuracy: 0.9919 - 4s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "\n",
            "Epoch 10: val_loss did not improve from 0.02899\n",
            "938/938 - 4s - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0453 - val_accuracy: 0.9912 - 4s/epoch - 4ms/step\n",
            "Epoch 1/10\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.03484, saving model to model.weights_CNN_2.best.hdf5\n",
            "938/938 - 4s - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.0348 - val_accuracy: 0.9904 - 4s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "\n",
            "Epoch 2: val_loss did not improve from 0.03484\n",
            "938/938 - 4s - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0420 - val_accuracy: 0.9892 - 4s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "\n",
            "Epoch 3: val_loss did not improve from 0.03484\n",
            "938/938 - 4s - loss: 0.0095 - accuracy: 0.9971 - val_loss: 0.0430 - val_accuracy: 0.9899 - 4s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "\n",
            "Epoch 4: val_loss did not improve from 0.03484\n",
            "938/938 - 4s - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.0412 - val_accuracy: 0.9902 - 4s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "\n",
            "Epoch 5: val_loss did not improve from 0.03484\n",
            "938/938 - 4s - loss: 0.0070 - accuracy: 0.9978 - val_loss: 0.0495 - val_accuracy: 0.9894 - 4s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.03484\n",
            "938/938 - 4s - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0401 - val_accuracy: 0.9907 - 4s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "\n",
            "Epoch 7: val_loss did not improve from 0.03484\n",
            "938/938 - 4s - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.0473 - val_accuracy: 0.9899 - 4s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.03484\n",
            "938/938 - 4s - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0550 - val_accuracy: 0.9892 - 4s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "\n",
            "Epoch 9: val_loss did not improve from 0.03484\n",
            "938/938 - 4s - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0529 - val_accuracy: 0.9909 - 4s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "\n",
            "Epoch 10: val_loss did not improve from 0.03484\n",
            "938/938 - 4s - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0523 - val_accuracy: 0.9903 - 4s/epoch - 4ms/step\n",
            "Epoch 1/10\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.05239, saving model to model.weights_CNN_3.best.hdf5\n",
            "938/938 - 8s - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0524 - val_accuracy: 0.9923 - 8s/epoch - 8ms/step\n",
            "Epoch 2/10\n",
            "\n",
            "Epoch 2: val_loss did not improve from 0.05239\n",
            "938/938 - 8s - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.0672 - val_accuracy: 0.9904 - 8s/epoch - 8ms/step\n",
            "Epoch 3/10\n",
            "\n",
            "Epoch 3: val_loss did not improve from 0.05239\n",
            "938/938 - 8s - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0580 - val_accuracy: 0.9920 - 8s/epoch - 8ms/step\n",
            "Epoch 4/10\n",
            "\n",
            "Epoch 4: val_loss did not improve from 0.05239\n",
            "938/938 - 8s - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0569 - val_accuracy: 0.9921 - 8s/epoch - 8ms/step\n",
            "Epoch 5/10\n",
            "\n",
            "Epoch 5: val_loss did not improve from 0.05239\n",
            "938/938 - 8s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0720 - val_accuracy: 0.9908 - 8s/epoch - 8ms/step\n",
            "Epoch 6/10\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.05239\n",
            "938/938 - 8s - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0688 - val_accuracy: 0.9919 - 8s/epoch - 8ms/step\n",
            "Epoch 7/10\n",
            "\n",
            "Epoch 7: val_loss did not improve from 0.05239\n",
            "938/938 - 8s - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0842 - val_accuracy: 0.9913 - 8s/epoch - 8ms/step\n",
            "Epoch 8/10\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.05239\n",
            "938/938 - 8s - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0782 - val_accuracy: 0.9911 - 8s/epoch - 8ms/step\n",
            "Epoch 9/10\n",
            "\n",
            "Epoch 9: val_loss did not improve from 0.05239\n",
            "938/938 - 8s - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0885 - val_accuracy: 0.9917 - 8s/epoch - 8ms/step\n",
            "Epoch 10/10\n",
            "\n",
            "Epoch 10: val_loss did not improve from 0.05239\n",
            "938/938 - 8s - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0774 - val_accuracy: 0.9921 - 8s/epoch - 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Model with the Best Classification Accuracy on the Validation Set"
      ],
      "metadata": {
        "id": "vpqAsUTZH0zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the weights that yielded the best validation accuracy\n",
        "modelCNN.load_weights('model.weights_CNN.best.hdf5')\n",
        "modelCNN_2.load_weights('model.weights_CNN_2.best.hdf5')\n",
        "modelCNN_3.load_weights('model.weights_CNN_3.best.hdf5')"
      ],
      "metadata": {
        "id": "k36Bn7jcGkbF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate the classification Accuracy on the Test Set"
      ],
      "metadata": {
        "id": "02xvefYaIJwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate test Accuracy\n",
        "score = modelCNN.evaluate(X_test,Y_test,verbose=0)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "# Print Test Accuracy\n",
        "print('Test Accuracy modelCNN: %.4f%%' % accuracy)\n",
        "\n",
        "score = modelCNN_2.evaluate(X_test,Y_test,verbose=0)\n",
        "accuracy = 100*score[1]\n",
        "# Print Test Accuracy\n",
        "print('Test Accuracy modelCNN2: %.4f%%' % accuracy)\n",
        "\n",
        "score = modelCNN_3.evaluate(X_test,Y_test,verbose=0)\n",
        "accuracy = 100*score[1]\n",
        "# Print Test Accuracy\n",
        "print('Test Accuracy modelCNN3: %.4f%%' % accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlMOOkzhIG0Z",
        "outputId": "ec9f07d4-675c-4933-b3b7-1b8681a5a23c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy modelCNN: 99.2800%\n",
            "Test Accuracy modelCNN2: 99.0400%\n",
            "Test Accuracy modelCNN3: 99.2300%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r73gPO1ZIks_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}